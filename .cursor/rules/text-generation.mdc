---
alwaysApply: true
---

# Gemini Text Generation - Cursor Rule

## Overview

The Gemini API generates text output from various inputs including text, images, video, and audio. This rule covers core text generation patterns, multimodal inputs, streaming, and conversation management.

## Core API Usage

### ‚úÖ CORRECT: Basic Text Generation

**JavaScript/TypeScript (Deno Edge Functions):**
```typescript
import { GoogleGenAI } from "https://esm.sh/@google/genai";

const ai = new GoogleGenAI({ apiKey: Deno.env.get('GEMINI_API_KEY') });

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: "How does AI work?",
});

console.log(response.text);
```

### ‚úÖ CORRECT: With Configuration

```typescript
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: "Explain how AI works",
  config: {
    temperature: 0.1, // Lower = more deterministic
    systemInstruction: "You are a helpful assistant.",
  },
});
```

## Content Structure

### ‚úÖ CORRECT: Simple Text Input

```typescript
// Single string (simplest)
contents: "How does AI work?"

// Array of strings
contents: ["First prompt", "Second prompt"]

// Content object with parts
contents: {
  parts: [{ text: "How does AI work?" }]
}

// Array of content objects (for multi-turn)
contents: [
  {
    role: "user",
    parts: [{ text: "Hello" }]
  },
  {
    role: "model",
    parts: [{ text: "Hi there!" }]
  }
]
```

### ‚úÖ CORRECT: Multimodal Inputs (Text + Image)

```typescript
import { createUserContent, createPartFromUri } from "@google/genai";

// Option 1: Upload file first, then use URI
const image = await ai.files.upload({
  file: "/path/to/image.png",
});

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: [
    createUserContent([
      "Tell me about this instrument",
      createPartFromUri(image.uri, image.mimeType),
    ]),
  ],
});

// Option 2: Inline base64 data (for Edge Functions)
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: {
    parts: [
      { text: "Tell me about this instrument" },
      {
        inlineData: {
          data: base64ImageString,
          mimeType: "image/png"
        }
      }
    ]
  }
});
```

## System Instructions

### ‚úÖ CORRECT: System Instruction Pattern

```typescript
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: "Hello there",
  config: {
    systemInstruction: "You are a cat. Your name is Neko.",
  },
});
```

**Best Practices:**
- Use system instructions to guide model behavior and persona
- Keep instructions clear and concise
- Define role, tone, and constraints in system instruction
- For structured outputs, include format requirements in system instruction

## Configuration Options

### ‚úÖ CORRECT: Generation Config

```typescript
const config = {
  // Temperature: Controls randomness (0.0-2.0, default ~1.0)
  // ‚ö†Ô∏è Gemini 3: Keep at default 1.0, changing may cause unexpected behavior
  temperature: 0.1, // Lower = more deterministic, higher = more creative

  // Top-P: Nucleus sampling (0.0-1.0)
  topP: 0.8,

  // Top-K: Limit vocabulary to top K tokens
  topK: 10,

  // System instruction
  systemInstruction: "You are a helpful assistant.",

  // Stop sequences (stop generation when these appear)
  stopSequences: ["Title", "---"],

  // Max output tokens
  maxOutputTokens: 1024,
};
```

### ‚ö†Ô∏è Temperature Warning (Gemini 3)

```typescript
// ‚ö†Ô∏è WARNING: For Gemini 3 models, keep temperature at default 1.0
// Changing temperature (especially below 1.0) may lead to:
// - Unexpected behavior
// - Looping
// - Degraded performance in complex mathematical/reasoning tasks

// ‚úÖ CORRECT for Gemini 2.5
config: {
  temperature: 0.1, // OK for 2.5
}

// ‚úÖ CORRECT for Gemini 3
config: {
  temperature: 1.0, // Keep at default for Gemini 3
}
```

## Thinking Configuration (Gemini 2.5)

### ‚úÖ CORRECT: Disable Thinking for Speed

```typescript
// Gemini 2.5 Flash and Pro have "thinking" enabled by default
// This enhances quality but increases latency and token usage

// Disable thinking for faster, lower-cost responses
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: "How does AI work?",
  config: {
    thinkingConfig: {
      thinkingBudget: 0, // Disables thinking
    },
  },
});
```

**When to disable thinking:**
- Simple, straightforward tasks
- High-throughput scenarios
- Cost-sensitive applications
- When latency is critical

**When to keep thinking enabled:**
- Complex reasoning tasks
- Multi-step problem solving
- When quality is more important than speed

## Streaming Responses

### ‚úÖ CORRECT: Streaming Pattern

```typescript
// Use streaming for better UX - receive chunks as they're generated
const stream = await ai.models.generateContentStream({
  model: "gemini-2.5-flash",
  contents: "Explain how AI works",
});

let fullResponse = "";
for await (const chunk of stream) {
  const text = chunk.text || "";
  fullResponse += text;
  // Process/display chunk immediately
  console.log(text);
}

// Full response available after stream completes
console.log("Complete:", fullResponse);
```

**Benefits:**
- Improved perceived performance
- Real-time feedback to users
- Better UX for long responses

**Use Cases:**
- Chat interfaces
- Long-form content generation
- Real-time applications

## Multi-Turn Conversations (Chat)

### ‚úÖ CORRECT: Chat Pattern

```typescript
// Create chat session
const chat = ai.chats.create({
  model: "gemini-2.5-flash",
  // Optional: Initialize with history
  history: [
    {
      role: "user",
      parts: [{ text: "Hello" }],
    },
    {
      role: "model",
      parts: [{ text: "Great to meet you. What would you like to know?" }],
    },
  ],
});

// Send messages
const response1 = await chat.sendMessage({
  message: "I have 2 dogs in my house.",
});
console.log(response1.text);

const response2 = await chat.sendMessage({
  message: "How many paws are in my house?",
});
console.log(response2.text);

// Get conversation history
const history = chat.getHistory();
for (const message of history) {
  console.log(`${message.role}: ${message.parts[0].text}`);
}
```

### ‚úÖ CORRECT: Streaming Chat

```typescript
const chat = ai.chats.create({
  model: "gemini-2.5-flash",
});

const stream = await chat.sendMessageStream({
  message: "I have 2 dogs in my house.",
});

for await (const chunk of stream) {
  console.log(chunk.text);
}
```

**Key Points:**
- Chat functionality is SDK-only (uses `generateContent` under the hood)
- Full conversation history is sent with each turn
- History is automatically managed by the SDK
- Use for conversational interfaces

## Response Parsing

### ‚úÖ CORRECT: Access Response Text

```typescript
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: "How does AI work?",
});

// Primary method: response.text
const text = response.text;

// Alternative: Access via candidates
const text = response.candidates?.[0]?.content?.parts?.[0]?.text;

// Handle empty responses
const text = response.text || "";
```

### ‚úÖ CORRECT: Handle Multiple Parts

```typescript
// Response may contain multiple parts (text, images, etc.)
const parts = response.candidates?.[0]?.content?.parts || [];

for (const part of parts) {
  if (part.text) {
    console.log("Text:", part.text);
  }
  if (part.inlineData) {
    console.log("Image:", part.inlineData.data);
  }
}
```

## Error Handling

### ‚úÖ CORRECT: Robust Error Handling

```typescript
try {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: prompt,
    config: {
      temperature: 0.1,
      systemInstruction: "You are a helpful assistant.",
    },
  });

  const text = response.text || "";
  
  if (!text || text.trim() === "") {
    throw new Error("Empty response from model");
  }

  return text;
} catch (error: any) {
  console.error("Text generation error:", error);
  
  // Handle specific error types
  if (error.message?.includes("API key")) {
    throw new Error("Invalid or missing API key");
  }
  if (error.message?.includes("quota")) {
    throw new Error("API quota exceeded");
  }
  
  throw new Error(`Failed to generate text: ${error.message}`);
}
```

## Best Practices

### ‚úÖ DO: Clear, Specific Prompts

```typescript
// ‚úÖ GOOD - Clear and specific
contents: "Extract the event title, date, and location from the following text: [text]"

// ‚ùå BAD - Too vague
contents: "Process this"
```

### ‚úÖ DO: Use System Instructions for Context

```typescript
// ‚úÖ GOOD - Define role and constraints
config: {
  systemInstruction: `
    You are an AI Event Architect for FashionOS.
    - Extract event details from user input
    - Use YYYY-MM-DD format for dates
    - Return structured JSON
  `,
}
```

### ‚úÖ DO: Use Streaming for Better UX

```typescript
// ‚úÖ GOOD - Streaming for long responses
const stream = await ai.models.generateContentStream({
  model: "gemini-2.5-flash",
  contents: prompt,
});
```

### ‚úÖ DO: Handle Multimodal Inputs Properly

```typescript
// ‚úÖ GOOD - Properly structure multimodal content
contents: {
  parts: [
    { text: "Describe this image" },
    {
      inlineData: {
        data: base64Image,
        mimeType: "image/png"
      }
    }
  ]
}
```

### ‚ùå DON'T: Expose API Keys

```typescript
// ‚ùå BAD - Never expose API keys in client code
const ai = new GoogleGenAI({ apiKey: "AIzaSy..." }); // Don't hardcode

// ‚úÖ GOOD - Use environment variables
const ai = new GoogleGenAI({ apiKey: Deno.env.get('GEMINI_API_KEY') });
```

### ‚ùå DON'T: Ignore Errors

```typescript
// ‚ùå BAD - No error handling
const response = await ai.models.generateContent({...});
return response.text; // May throw

// ‚úÖ GOOD - Proper error handling
try {
  const response = await ai.models.generateContent({...});
  return response.text || "";
} catch (error) {
  console.error("Generation failed:", error);
  throw error;
}
```

## Model Selection

### Recommended Models for Text Generation

| Model | Use Case | Speed | Quality |
|-------|----------|-------|---------|
| `gemini-2.5-flash` | General purpose, fast responses | ‚ö° Fast | ‚≠ê‚≠ê‚≠ê Good |
| `gemini-2.5-pro` | Complex reasoning, high quality | üê¢ Slower | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent |
| `gemini-3-pro-preview` | Advanced reasoning, latest features | üê¢ Slower | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent |

**Default Recommendation:** `gemini-2.5-flash` for most use cases

## Common Patterns

### Pattern 1: Simple Text Generation

```typescript
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: userPrompt,
});
return response.text;
```

### Pattern 2: Text with System Instruction

```typescript
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: userPrompt,
  config: {
    systemInstruction: "You are a helpful assistant.",
    temperature: 0.1,
  },
});
return response.text;
```

### Pattern 3: Multimodal (Text + Image)

```typescript
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: {
    parts: [
      { text: "Describe this image" },
      {
        inlineData: {
          data: base64Image,
          mimeType: "image/png"
        }
      }
    ]
  }
});
return response.text;
```

### Pattern 4: Streaming

```typescript
const stream = await ai.models.generateContentStream({
  model: "gemini-2.5-flash",
  contents: prompt,
});

let fullText = "";
for await (const chunk of stream) {
  fullText += chunk.text || "";
  // Process chunk
}
return fullText;
```

### Pattern 5: Multi-Turn Chat

```typescript
const chat = ai.chats.create({
  model: "gemini-2.5-flash",
});

const response = await chat.sendMessage({
  message: userMessage,
});
return response.text;
```

## Current Implementation Status

### ‚úÖ Correct Implementations
- `resolve-venue/index.ts:47` - Uses `generateContent` with proper model ‚úÖ
- `generate-image-preview/index.ts:44` - Uses `generateContent` for text analysis ‚úÖ
- Proper error handling in edge functions ‚úÖ

### ‚ö†Ô∏è Potential Improvements
- Consider using streaming for better UX
- Add system instructions where appropriate
- Consider chat sessions for multi-turn interactions
- Add thinking budget configuration for cost optimization

## Quick Reference

**Basic Generation:**
- `ai.models.generateContent({ model, contents, config })`
- Access text via `response.text`
- Always handle empty responses

**Streaming:**
- `ai.models.generateContentStream({ model, contents, config })`
- Iterate with `for await (const chunk of stream)`
- Access text via `chunk.text`

**Chat:**
- `ai.chats.create({ model, history })`
- `chat.sendMessage({ message })`
- `chat.getHistory()` for conversation history

**Configuration:**
- `temperature`: 0.0-2.0 (keep 1.0 for Gemini 3)
- `systemInstruction`: Guide model behavior
- `thinkingConfig.thinkingBudget`: 0 to disable thinking (2.5 only)

**Model Selection:**
- `gemini-2.5-flash`: Default for most use cases
- `gemini-2.5-pro`: Complex reasoning
- `gemini-3-pro-preview`: Latest features

## When Implementing Text Generation

1. **Choose the right model** - Flash for speed, Pro for quality
2. **Use system instructions** - Guide model behavior and persona
3. **Configure temperature** - Lower for deterministic, higher for creative
4. **Consider streaming** - For better UX with long responses
5. **Handle errors gracefully** - Always wrap in try-catch
6. **Use chat for conversations** - Better than manual history management
7. **Optimize thinking budget** - Disable for simple tasks (2.5 only)
8. **Validate responses** - Check for empty or invalid output
9. **Use multimodal inputs** - Combine text with images/video/audio when needed
10. **Follow best practices** - Clear prompts, proper error handling, secure API keys
